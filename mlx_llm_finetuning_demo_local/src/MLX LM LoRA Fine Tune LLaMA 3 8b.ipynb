{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1055c3f3",
   "metadata": {},
   "source": [
    "### LoRA Fine-Tuning with MLX LM\n",
    "\n",
    "In this notebook, we'll walk through how to [LoRA fine-tune](https://arxiv.org/abs/2106.09685) an LLM with MLX LM. We'll use the [HellaSwag](https://rowanzellers.com/hellaswag/) dataset for common sense reasoning as an example. An outline:\n",
    "\n",
    "1. Download the dataset and prepare it in the right format for MLX LM.\n",
    "2. Setup and run LoRA training. We'll show how to capture the training logs and plot some statistics to visualize the performance.\n",
    "3. Evaluate on the test set. We'll compute the final question-answer accuracy of the fine-tuned model.\n",
    "4. Fuse the resulting adapters into the base model and upload to Hugging Face.\n",
    "5. Discuss tips for debugging accuracy and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b566b94b",
   "metadata": {},
   "source": [
    "If you have not yet initialized the environment the following block will do so, but only if it does not already exist! Make sure to use this conda environment after for the fine tuning! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90ef0322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if the Conda environment 'mlx_llm_tuner' exists...\n",
      "Conda environment 'mlx_llm_tuner' already exists.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "# Define environment details\n",
    "conda_env_name = \"mlx_llm_tuner\"\n",
    "python_version = \"3.12\"\n",
    "\n",
    "# Step: Create the Conda Environment if it does not exist\n",
    "try:\n",
    "    # Check if the conda environment exists\n",
    "    print(f\"Checking if the Conda environment '{conda_env_name}' exists...\")\n",
    "    env_list = subprocess.run([\"conda\", \"env\", \"list\"], capture_output=True, text=True)\n",
    "    if conda_env_name not in env_list.stdout:\n",
    "        print(f\"Environment '{conda_env_name}' not found. Creating it with Python {python_version}...\")\n",
    "        subprocess.run(\n",
    "            [\n",
    "                \"conda\", \"create\", \"--name\", conda_env_name,\n",
    "                f\"python={python_version}\", \"jupyter\", \"-y\"\n",
    "            ],\n",
    "            check=True\n",
    "        )\n",
    "        print(f\"Conda environment '{conda_env_name}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Conda environment '{conda_env_name}' already exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error setting up Conda environment: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21397627",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "664272fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install mlx-lm\n",
    "#!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd27c693",
   "metadata": {},
   "source": [
    "### Preprocess Data\n",
    "We'll start by downloading an already pre-processed version of the HellaSwag dataset from [LLM-Adapters](https://github.com/AGI-Edgerunners/LLM-Adapters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "839b4dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from urllib import request\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8da1c8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory (in a Jupyter notebook this will be the notebook's directory).\n",
    "script_dir = os.path.abspath('.')  # If in a .py file, use: os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# Define the save directory relative to the current directory\n",
    "save_dir = os.path.join(script_dir, '..', 'data', 'framing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61698208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HellaSwag stats: 39905 training examples and 10042 test examples.\n",
      "An example:\n",
      "\n",
      "{\n",
      "    \"instruction\": \"Please choose the correct ending to complete the given sentence: Removing ice from car: Then, the man writes over the snow covering the window of a car, and a woman wearing winter clothes smiles. then\\n\\nEnding1: , the man adds wax to the windshield and cuts it. Ending2: , a person board a ski lift, while two men supporting the head of the person wearing winter clothes snow as the we girls sled. Ending3: , the man puts on a christmas coat, knitted with netting. Ending4: , the man continues removing the snow on his car.\\n\\nAnswer format: ending1/ending2/ending3/ending4\",\n",
      "    \"input\": \"\",\n",
      "    \"output\": \"the correct answer is ending4\",\n",
      "    \"answer\": \"ending4\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def download_and_save(save_dir):\n",
    "    base_url = \"https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/main/dataset/hellaswag/\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    for name in [\"train.json\", \"test.json\"]:\n",
    "        out_file = os.path.join(save_dir, name)\n",
    "        if not os.path.exists(out_file):\n",
    "            request.urlretrieve(base_url + name, out_file)\n",
    "\n",
    "def load_json(dataset):\n",
    "    download_and_save(save_dir)\n",
    "    with open(os.path.join(save_dir, f\"{dataset}.json\"), \"r\") as fid:\n",
    "        return json.load(fid)\n",
    "\n",
    "train_set = load_json(\"train\")\n",
    "test_set = load_json(\"test\")\n",
    "\n",
    "print(f\"HellaSwag stats: {len(train_set)} training examples and {len(test_set)} test examples.\")\n",
    "print(\"An example:\\n\")\n",
    "print(json.dumps(train_set[0], indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a514d79",
   "metadata": {},
   "source": [
    "Next, let's split the training set into a training and a validation set. We'll pull out a randomly chosen 10% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b607237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for reproducibility\n",
    "np.random.seed(43)\n",
    "perm = np.random.permutation(len(train_set))\n",
    "valid_size = int(0.1 * len(train_set))\n",
    "valid_set = [train_set[i] for i in perm[:valid_size]]\n",
    "train_set = [train_set[i] for i in perm[valid_size:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c38c4e",
   "metadata": {},
   "source": [
    "Finally, put the data splits in the MLX LM training format. The format simply expects the data to be in a container which supports random access to the individual examples (e.g. a Python `list`):\n",
    "```\n",
    "[\"An example for the model.\", \"Another example for the model.\", ...]\n",
    "```\n",
    "For more details, see the [documentation on supported formats](https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/LORA.md#Data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea738f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset):\n",
    "    return [t[\"instruction\"] + \"\\n\" + t[\"output\"] for t in dataset]\n",
    "\n",
    "train_set, valid_set, test_set = map(preprocess, (train_set, valid_set, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "366137d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please choose the correct ending to complete the given sentence: Youth: [header] How to fit in with the cool kids [title] Go to sporting events. [step] Even if you're not athletically gifted, you can find friends through athletics. Sitting front row at your school's big games not only puts you around other cool kids, but it also gives you common ground.\n",
      "\n",
      "Ending1: If you're a very bright kid, many other kids are too. [substeps] You also have the opportunity to also befriend sports fans. Ending2: It gives you things to talk about the next day at school. [substeps] If you are athletically gifted-try out for the team. Ending3: [title] Use your clique as a support. [step] Sometimes, you'll just need your clique. Ending4: [substeps] If you're feeling daring, try being the fourth person at your school that meets the age minimum. This will give you an edge, and it might actually be a lot more fun if you were already a cool kid.\n",
      "\n",
      "Answer format: ending1/ending2/ending3/ending4\n",
      "the correct answer is ending2\n"
     ]
    }
   ],
   "source": [
    "print(train_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b259eb69",
   "metadata": {},
   "source": [
    "### Fine-Tune\n",
    "\n",
    "For fine-tuning, we'll use Microsoft's [LlaMA 3.1 8b](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct). At 8 billion parameters, LlaMA 3.1 8b is a high-quality model that is also fast to fine-tune on most Apple silicon machines.\n",
    "\n",
    "First, import all the packages and functions we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3ff309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlx.core as mx\n",
    "import mlx.optimizers as optim\n",
    "from mlx.utils import tree_flatten\n",
    "from mlx_lm import load, generate\n",
    "from mlx_lm.tuner import train, evaluate, TrainingArgs\n",
    "from mlx_lm.tuner import linear_to_lora_layers\n",
    "import mlx.nn as nn\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87628d24",
   "metadata": {},
   "source": [
    "Next, setup the LoRA parameters and make the training arguments. See the [training argument class](https://github.com/ml-explore/mlx-examples/blob/81318ad4a8b2ca5fd1431a42db2b0244d16be851/llms/mlx_lm/tuner/trainer.py#L31-L63) for a more detailed list of training parameters. \n",
    "\n",
    "Recall the LoRA update is $W^\\top \\mathbf{x} + c \\cdot \\mathbf{a} \\mathbf{b}^\\top \\mathbf{x}$ where $\\mathbf{a}$ has shape `(D, rank)`.\n",
    "\n",
    "With that in mind, the LoRA parameters to attend to are:\n",
    "- `lora_layers`: The number of Transformer blocks from the top of the model to adapt.\n",
    "- `rank`: The rank of the low-rank adapters. A larger rank implies more adapter parameters per linear layer.\n",
    "- `scale`: This is the constant $c$ that scales the low-rank update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84f3b083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "script_dir = os.path.abspath('.')\n",
    "\n",
    "# Define the adapter directory relative to the current directory\n",
    "adapter_path = os.path.join(script_dir, '..', 'adapters','testing')\n",
    "\n",
    "# Make a directory to save the adapter config and weights, only if it does not exist\n",
    "if not os.path.exists(adapter_path):\n",
    "    os.makedirs(adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0851dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = {\n",
    "    \"lora_layers\": 8,\n",
    "    \"lora_parameters\": {\n",
    "        \"rank\": 8,\n",
    "        \"scale\": 20.0,\n",
    "        \"dropout\": 0.0\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save the LoRA config to the adapter path\n",
    "adapter_config_file = os.path.join(adapter_path, \"adapter_config.json\")\n",
    "with open(adapter_config_file, \"w\") as fid:\n",
    "    json.dump(lora_config, fid, indent=4)\n",
    "\n",
    "# TrainingArgs is defined elsewhere in your code:\n",
    "training_args = TrainingArgs(\n",
    "    adapter_file=os.path.join(adapter_path, \"adapters.safetensors\"),\n",
    "    iters=200,\n",
    "    steps_per_eval=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fefd19",
   "metadata": {},
   "source": [
    "Next, load the LlaMA-3.1 8b model. Note this may take a few minutes to download from HuggingFace if you haven't downloaded it before. Moreovere you need to have access to the model and register your Huggingface account with huggingface CLI (see here: https://huggingface.co/docs/huggingface_hub/main/en/guides/cli) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb0b16f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 11 files: 100%|██████████| 11/11 [00:00<00:00, 119837.26it/s]\n"
     ]
    }
   ],
   "source": [
    "model_path = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "model, tokenizer = load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdae9d9",
   "metadata": {},
   "source": [
    "Now we could change the Quantization of the model to 8 Bit or something similar and train the model with this quantization. I generally would not do that since if possible train it in fp 16 or fp 32. The reason is to be able to run the fine-tuned model with ollama we will have to convert the model to a 'gguf' format. This is only supported for models that are not quantized. Hence we would have to de-quantize the fine-tuned model at the end before transforming it to a 'gguf' with llama.cpp. I advise to use llama.cpp to do the conversions and not with the mlx converter as it is generally easier to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f1c7693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your desired quantization parameters\n",
    "#q_bits = 8       # For 8-bit quantization\n",
    "#q_group_size = 64\n",
    "\n",
    "# Quantize the model in-place\n",
    "#nn.quantize(model, q_group_size, q_bits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6609c92a",
   "metadata": {},
   "source": [
    "After loading the model, freeze it's parameters so we don't train them. Then convert linear layers to LoRA layers using the MLX LM utility `linear_to_lora_layers`. The adapters in the `LoRA` layers are not frozen, so they will be included in the model's `trainable_parameters`. Check-out the [LoRA layer implementation](https://github.com/ml-explore/mlx-examples/blob/81318ad4a8b2ca5fd1431a42db2b0244d16be851/llms/mlx_lm/tuner/lora.py#L72-L104) to see how it all works.\n",
    "\n",
    "By default, MLX LM only adapts the query, key, and value projection matrices for Phi-3. You can specify the layers to adapt by setting `lora_parameters[\"keys\"]` to a list of layer names. In this case it defaults to `[\"attn.qkv_proj\"]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50e1ab3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 3407872\n"
     ]
    }
   ],
   "source": [
    "# Freeze the base model\n",
    "model.freeze()\n",
    "\n",
    "# Convert linear layers to lora layers\n",
    "linear_to_lora_layers(model, lora_config[\"lora_layers\"], lora_config[\"lora_parameters\"])\n",
    "\n",
    "num_train_params = (\n",
    "    sum(v.size for _, v in tree_flatten(model.trainable_parameters()))\n",
    ")\n",
    "print(f\"Number of trainable parameters: {num_train_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d1590",
   "metadata": {},
   "source": [
    "Now we're ready to put it all together and actually train the model. We'll use `Adam` for the optimizer, but you can specify any [optimizer](https://ml-explore.github.io/mlx/build/html/python/optimizers/common_optimizers.html) with any [scheduler](https://ml-explore.github.io/mlx/build/html/python/optimizers/schedulers.html). We also added a custom class to capture the training and validation loss to plot it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984516d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Put the model in training mode:\n",
    "model.train()\n",
    "\n",
    "# Make the optimizer:\n",
    "opt = optim.Adam(learning_rate=1e-5)\n",
    "\n",
    "# Make a class to record the training stats:\n",
    "class Metrics:\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    def on_train_loss_report(self, info):\n",
    "        self.train_losses.append((info[\"iteration\"], info[\"train_loss\"]))\n",
    "    def on_val_loss_report(self, info):\n",
    "        self.val_losses.append((info[\"iteration\"], info[\"val_loss\"]))\n",
    "\n",
    "metrics = Metrics()\n",
    "\n",
    "# Train model:\n",
    "train(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    optimizer=opt,\n",
    "    train_dataset=train_set,\n",
    "    val_dataset=valid_set,\n",
    "    training_callback=metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d043b8",
   "metadata": {},
   "source": [
    "The adapters are saved every 100 iterations along with the final adapters in `adapters.safetensors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac329358",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_path = os.path.join(script_dir, '..', 'adapters', 'framing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "367ff4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000100_adapters.safetensors adapter_config.json\n",
      "0000200_adapters.safetensors adapters.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls ../adapters/framing/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7e23ee",
   "metadata": {},
   "source": [
    "Next, let's plot the training and validation losses to see how well the adapters fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1ffd638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_its, train_losses = zip(*metrics.train_losses)\n",
    "val_its, val_losses = zip(*metrics.val_losses)\n",
    "plt.plot(train_its, train_losses, '-o')\n",
    "plt.plot(val_its, val_losses, '-o')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(['Train', \"Valid\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28f216c",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "The training and validation loss are only part of the story. For HellaSwag, we ultimately care about how good the model is at answering questions. To asses this, let's generate the actual `ending1`, `ending2`, `ending3`, or `ending4` responses with the fine-tuned model and measure the accuracy.\n",
    "\n",
    "First, let's split the last word off of each example in the test set to create a prompt without the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d96e4dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [t.rsplit(\" \", maxsplit=1) for t in test_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8becd26a",
   "metadata": {},
   "source": [
    "Next, we'll generate the response for each example in the test set and compare it to the ground-truth answer to measure the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b396980a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:54<00:00,  1.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# Increase this number to use more test examples\n",
    "num_test = 150\n",
    "num_correct = 0\n",
    "for prompt, answer in tqdm.tqdm(test_set[:num_test]):\n",
    "    response = generate(model, tokenizer, prompt, max_tokens=2)\n",
    "    num_correct += (response==answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cbc00b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate test accuracy 0.833\n"
     ]
    }
   ],
   "source": [
    "test_acc = num_correct / num_test\n",
    "print(f\"Approximate test accuracy {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fbba7f",
   "metadata": {},
   "source": [
    "### Fuse Adapters\n",
    "\n",
    "Sometimes its convenient to fuse the adapters into the base model to create a single adapted model. MLX LM has a fuse script just for that.\n",
    "\n",
    "The adapted weights are: $\\tilde{W} = W + c \\cdot \\mathbf{b}^\\top \\mathbf{a}$. Note, this process can be destructive if the inputs are in low precision and they have very different magnitudes. Tuning the `scale` parameter, $c$, prior to fine-tuning can improve the model performance after fusion.\n",
    "\n",
    "To see more options for fusing the model, including how to upload to HuggingFace [check the documentation](https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/LORA.md#fuse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40915642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n",
      "usage: mlx_lm.fuse [-h] [--model MODEL] [--save-path SAVE_PATH]\n",
      "                   [--adapter-path ADAPTER_PATH] [--hf-path HF_PATH]\n",
      "                   [--upload-repo UPLOAD_REPO] [--de-quantize] [--export-gguf]\n",
      "                   [--gguf-path GGUF_PATH]\n",
      "\n",
      "Fuse fine-tuned adapters into the base model.\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --model MODEL         The path to the local model directory or Hugging Face\n",
      "                        repo.\n",
      "  --save-path SAVE_PATH\n",
      "                        The path to save the fused model.\n",
      "  --adapter-path ADAPTER_PATH\n",
      "                        Path to the trained adapter weights and config.\n",
      "  --hf-path HF_PATH     Path to the original Hugging Face model. Required for\n",
      "                        upload if --model is a local directory.\n",
      "  --upload-repo UPLOAD_REPO\n",
      "                        The Hugging Face repo to upload the model to.\n",
      "  --de-quantize         Generate a de-quantized model.\n",
      "  --export-gguf         Export model weights in GGUF format.\n",
      "  --gguf-path GGUF_PATH\n",
      "                        Path to save the exported GGUF format model weights.\n",
      "                        Default is ggml-model-f16.gguf.\n",
      "/Users/Mael/DigDemLab/Archive/digdemlab_finetuning_llama_ollama/src\n"
     ]
    }
   ],
   "source": [
    "!mlx_lm.fuse --help\n",
    "\n",
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f6bfb548",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = {\n",
    "    \"model_type\": \"llama\",\n",
    "    \"num_layers\": 32,\n",
    "    \"lora_layers\": 8,\n",
    "    \"lora_parameters\": {\n",
    "        \"rank\": 8,\n",
    "        \"scale\": 20.0,\n",
    "        \"dropout\": 0.0\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save the LoRA config to the adapter path\n",
    "adapter_config_file = os.path.join(adapter_path, \"adapter_config.json\")\n",
    "with open(adapter_config_file, \"w\") as fid:\n",
    "    json.dump(lora_config, fid, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37854c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n",
      "Fetching 11 files: 100%|█████████████████████| 11/11 [00:00<00:00, 55587.16it/s]\n"
     ]
    }
   ],
   "source": [
    "!mlx_lm.fuse --model {model_path} --adapter-path ../adapters/framing --save-path ../adapters/framing/lora_fused_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c349707e",
   "metadata": {},
   "source": [
    "Once the adapters are fused, we can rerun the evaluation using the fused model to make sure it worked. By default the fused model will be saved to `lora_fused_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c1c45e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:52<00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate test accuracy 0.833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load(\"../adapters/framing/lora_fused_model\")\n",
    "num_correct = 0\n",
    "for prompt, answer in tqdm.tqdm(test_set[:num_test]):\n",
    "    response = generate(model, tokenizer, prompt, max_tokens=2)\n",
    "    num_correct += (response==answer)\n",
    "test_acc = num_correct / num_test\n",
    "print(f\"Approximate test accuracy {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dc7f4c",
   "metadata": {},
   "source": [
    "### Troubleshooting\n",
    "\n",
    "#### Results\n",
    "\n",
    "To figure out why your LoRA adapters are not working well it's critical to plot both the trianing loss and validation loss over the duration of fine-tuning. There are really only two cases to consider: underfitting or overfitting. And you can figure out which regime you are in based on the above plot.\n",
    "\n",
    "**Underfitting**: The trianing loss is not low enough and the validation loss closely matches the training loss. You could also measure the accuracy on the training set itself for question-answering style tasks like HellaSwag. If you are in this regime you have a few options to improve the results:\n",
    "\n",
    "- Use more adapters. Increase `lora_layers` or adapt more of the linear layers within a given block by setting `lora_parameters[\"keys\"]`.\n",
    "- Use a higher rank. A higher rank means more parameters per adapter.\n",
    "- If you are using dropout, decrease the droupout rate or turn it off entirely.\n",
    "- Sometimes, underfitting issues are really optimization issues. In these cases it can be helpful to tune the learning rate or learning rate schedule.\n",
    "- If none of the above works, try a bigger model.\n",
    "\n",
    "**Overfitting**: The trianing loss keeps going down but the validation loss stops going down and even starts to go up. If you are in this regime you also have a few options:\n",
    "\n",
    "- The best thing to do is to use more trianing data if you have it.\n",
    "- Contrary to the underfitting regime decreasing the capacity of the model can help. For example, use fewer adapters, a lower LoRA rank, or a smaller model size.\n",
    "- If you are not using dropout, use it.\n",
    "\n",
    "If you find your adapters work well pre-fusion but stop working post-fusion, try tuning the `scale` parameter, $c$, prior to fine-tuning. Typically the adapters have a smaller magnitude than the weights, so using a larger scale helps.\n",
    "\n",
    "#### Memory Use\n",
    "\n",
    "Fine-tuning a large LM with LoRA requires a machine with a decent amount of memory. Here are some tips to reduce memory use should you need to do so. \n",
    "\n",
    "- Try quantization (QLoRA). You can use QLoRA by generating a quantized model with `mlx_lm.convert` and the `-q` flag or by using an already quantized model from HuggingFace.\n",
    "\n",
    "- Try using a smaller batch size. You can set the `batch_size` parameter in the `TrainingArgs` or pass `--batch-size` if you are using the CLI. The default is 4 so setting this to 2 or 1 will reduce memory consumption. Note, this may slow things down a little..\n",
    "\n",
    "- Reduce the number of layers to fine-tune with by setting `lora_layers` to a smaller value or passing `--lora-layers` if you are using the CLI. The default is `16`, so you can try `8` or `4`. This reduces the amount of memory needed for back propagation. It may also reduce the quality of the fine-tuned model and you may need to compensate with a larger `rank`.\n",
    "\n",
    "- Longer examples require more memory. If it makes sense for your data, one thing you can do is break your examples into smaller sequences when making the `train`, `valid`, and `test` data sets.\n",
    "\n",
    "- Gradient checkpointing lets you trade-off memory use (less) for computation (more) by recomputing instead of storing intermediate values needed by the backward pass. You can use gradient checkpointing by passing `grad_checkpoint=True` to the `TrainingArgs` or the `--grad-checkpoint` flag if using the CLI. Gradient checkpointing will be more helpful for larger batch sizes or sequence lengths with smaller or quantized models.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- To learn more about MLX check-out the [GitHub repo](http://github.com/ml-explore/mlx) and [documentation](https://ml-explore.github.io/mlx/)\n",
    "- For more on MLX LM check-out the [MLX LM documentation](https://github.com/ml-explore/mlx-examples/tree/main/llms#readme).\n",
    "- Check out the other [MLX Examples](https://github.com/ml-explore/mlx-examples/tree/main). These are great as a learning resource or to use as a starting point for a new project.\n",
    "- We also have an example of [LoRA fine-tuning in MLX Swift](https://github.com/ml-explore/mlx-swift-examples/tree/main/Applications/LoRATrainingExample)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx_llm_tuner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
